"""Script to unify perturbation IDs"""
import argparse
from pathlib import Path
from functools import partial

import orjson
import pandas as pd
from tqdm.auto import tqdm
from tqdm.contrib.concurrent import process_map

from loader import load_plate
from id_mapping import JCP_MAPPER
from nan_filling import fillna
from jump.utils import CONFIG


def get_jcpid(row: pd.Series):
    """Try to map ID columns to a JCP_MAPPER code. If it is not found, it will
    return a json representation of this row for debugging purposes"""
    col_id_opts = [
        "jump-identifier",
        "Metadata_jump-identifier",
        "broad_sample",
        "Metadata_broad_sample",
    ]
    for col_id in col_id_opts:
        if col_id in row:
            key = str(row[col_id])  # Deal with None, convert it to string
            if key in JCP_MAPPER:
                return JCP_MAPPER[key]
            if key.startswith("JCP2022") and key in JCP_MAPPER.values():
                return key

    return row[[c for c in row.index if isinstance(row[c], str)]].to_json()


def assert_jcp_completed(plate: pd.DataFrame):
    """Assert every well in this plate has a valid ID to map"""
    plate_id = plate["Metadata_Plate"].iloc[0]
    if "jump-identifier" in plate and plate["jump-identifier"].isna().any():
        raise ValueError(f"Plate {plate_id} has missing jump-identifier")

    if "broad_sample" in plate and plate["broad_sample"].isna().any():
        raise ValueError(f"Plate {plate_id} has missing broad_sample")

    col_id_opts = [
        "jump-identifier",
        "Metadata_jump-identifier",
        "broad_sample",
        "Metadata_broad_sample",
    ]
    if all(col not in plate for col in col_id_opts):
        raise ValueError(f"Plate {plate_id} does not have ID column")


def process_plate(plate_props, source_id, cpg_id):
    """Load profile, fill NaN values and find JCPIDs"""
    plate = load_plate(plate_props, "default")
    fillna(plate, plate_props, source_id)
    try:
        assert_jcp_completed(plate)
    except ValueError as ex:
        return plate_props, str(ex)
    plate["Metadata_CPGID"] = cpg_id
    plate["Metadata_Source"] = source_id
    plate["Metadata_JCP2022"] = plate.apply(get_jcpid, axis=1)
    plate = plate[
        ["Metadata_Source", "Metadata_Plate", "Metadata_Well", "Metadata_JCP2022"]
    ]
    return plate.astype(str).copy()


def map_all_datasets(dataset_paths, output_path):
    """Main loop to process all dataset"""
    errors = []
    metadata = []
    for jsonfile in tqdm(dataset_paths, desc="datasets"):
        with open(jsonfile, "rb") as fread:
            dataset = orjson.loads(fread.read())
        cpg_id = CONFIG["cpg_id"]
        dataset_id = dataset["dataset_id"]
        for batch in tqdm(dataset["batches"], leave=False, desc=dataset_id):
            par_func = partial(process_plate, source_id=dataset_id, cpg_id=cpg_id)
            for plate in process_map(par_func, batch["plates"], leave=False):
                if isinstance(plate, pd.DataFrame):
                    metadata.append(plate)
                else:
                    plate_props, msg = plate
                    errors.append(
                        {
                            "platemap": plate_props["platemap"]["path"],
                            "barcode": batch["barcode_platemap"]["path"],
                            "profile": plate_props["profiles"]["default"]["path"],
                            "message": msg,
                        }
                    )

    errors = pd.DataFrame(errors)
    metadata = pd.concat(metadata)
    missing = metadata[~metadata["Metadata_JCP2022"].str.startswith("JCP")]
    codes = metadata[metadata["Metadata_JCP2022"].str.startswith("JCP")]

    data_dir = Path(output_path)
    errors.to_csv(data_dir / "well_errors.csv.gz", index=False, compression="gzip")
    codes.to_csv(data_dir / "well.csv.gz", index=False, compression="gzip")
    missing.to_csv(data_dir / "well_missing.csv.gz", index=False, compression="gzip")


def main():
    """Parse input params"""
    parser = argparse.ArgumentParser(
        description=("Create the collated metadata file using multiple datasets"),
    )
    parser.add_argument(
        "dataset_paths",
        metavar="JSONFILE",
        nargs="+",
        help="json file generated by create_structure script",
    )
    parser.add_argument(
        "--output_path",
        help="directory to save the collated file",
        default="./outputs/",
    )

    args = parser.parse_args()
    map_all_datasets(args.dataset_paths, args.output_path)


if __name__ == "__main__":
    main()
